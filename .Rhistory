sum(y^2)
t(y)%*%bX%*%mu_w
dim(mu_w)
source("~/Documents/GitHub/BML_Trento_May2024/R/vblasso.R")
# Define prior parameters
priors = list(s=1, a_sigma = 2, b_sigma = 1, s2_0 = 10)
# Define initialization
init = list(mu_w = lm_prostate$coefficients, Sigma_w = diag(1,D+1),
a_sigma = priors$a_sigma, b_sigma = priors$b_sigma, mu_tau = 2*priors$s^2)
I = 100 #number of iterations
thresh = 0.001 # threshold for change in elbo
#Run MCMC
vblasso_out = vblasso(X,y, priors, I, thresh, init)
t(y)%*%bX
- 2*t(y)%*%bX%*%mu_w
(Sigma_w + mu_w%*%t(mu_w))
(bX%*%t(bX)+ diag(c(1/s2_0,mu_tau))
)
bX %*% t(bX)
source("~/Documents/GitHub/BML_Trento_May2024/R/vblasso.R")
# Define prior parameters
priors = list(s=1, a_sigma = 2, b_sigma = 1, s2_0 = 10)
# Define initialization
init = list(mu_w = lm_prostate$coefficients, Sigma_w = diag(1,D+1),
a_sigma = priors$a_sigma, b_sigma = priors$b_sigma, mu_tau = 2*priors$s^2)
I = 100 #number of iterations
thresh = 0.001 # threshold for change in elbo
#Run MCMC
vblasso_out = vblasso(X,y, priors, I, thresh, init)
t(bX)%*%bX
diag(c(1/s2_0,mu_tau))
source("~/Documents/GitHub/BML_Trento_May2024/R/vblasso.R")
# Define prior parameters
priors = list(s=1, a_sigma = 2, b_sigma = 1, s2_0 = 10)
# Define initialization
init = list(mu_w = lm_prostate$coefficients, Sigma_w = diag(1,D+1),
a_sigma = priors$a_sigma, b_sigma = priors$b_sigma, mu_tau = rep(2*priors$s^2,D))
I = 100 #number of iterations
thresh = 0.001 # threshold for change in elbo
#Run MCMC
vblasso_out = vblasso(X,y, priors, I, thresh, init)
diag(c(1/s2_0,mu_tau))
(t(bX)%*%bX+ diag(c(1/s2_0,mu_tau)))
sum((Sigma_w + mu_w%*%t(mu_w))*(t(bX)%*%bX+ diag(c(1/s2_0,mu_tau))))
source("~/Documents/GitHub/BML_Trento_May2024/R/vblasso.R")
# Define prior parameters
priors = list(s=1, a_sigma = 2, b_sigma = 1, s2_0 = 10)
# Define initialization
init = list(mu_w = lm_prostate$coefficients, Sigma_w = diag(1,D+1),
a_sigma = priors$a_sigma, b_sigma = priors$b_sigma, mu_tau = rep(2*priors$s^2,D))
I = 100 #number of iterations
thresh = 0.001 # threshold for change in elbo
#Run MCMC
vblasso_out = vblasso(X,y, priors, I, thresh, init)
b_sigma_vb/(a_sigma_vb-1)
# Define prior parameters
priors = list(s=1, a_sigma = 2, b_sigma = 1, s2_0 = 10)
# Define initialization
init = list(mu_w = lm_prostate$coefficients, Sigma_w = diag(1,D+1),
a_sigma = priors$a_sigma, b_sigma = priors$b_sigma, mu_tau = rep(2*priors$s^2,D))
I = 100 #number of iterations
thresh = 0.001 # threshold for change in elbo
#Run MCMC
vblasso_out = vblasso(X,y, priors, I, thresh, init)
diag(Sigma_w)
mu_w^2
(diag(Sigma_w) + mu_w^2)
s^2*(diag(Sigma_w) + mu_w^2)
a_sigma_vb/b_sigma_vb
s^2*(diag(Sigma_w) + mu_w^2)*a_sigma_vb/b_sigma_vb
source("~/Documents/GitHub/BML_Trento_May2024/R/vblasso.R")
# Define prior parameters
priors = list(s=1, a_sigma = 2, b_sigma = 1, s2_0 = 10)
# Define initialization
init = list(mu_w = lm_prostate$coefficients, Sigma_w = diag(1,D+1),
a_sigma = priors$a_sigma, b_sigma = priors$b_sigma, mu_tau = rep(2*priors$s^2,D))
I = 100 #number of iterations
thresh = 0.001 # threshold for change in elbo
#Run MCMC
vblasso_out = vblasso(X,y, priors, I, thresh, init)
1/sqrt(s^2*(diag(Sigma_w) + mu_w^2)*a_sigma_vb/b_sigma_vb)
# Define prior parameters
priors = list(s=1, a_sigma = 2, b_sigma = 1, s2_0 = 10)
# Define initialization
init = list(mu_w = lm_prostate$coefficients, Sigma_w = diag(1,D+1),
a_sigma = priors$a_sigma, b_sigma = priors$b_sigma, mu_tau = rep(2*priors$s^2,D))
I = 100 #number of iterations
thresh = 0.001 # threshold for change in elbo
#Run MCMC
vblasso_out = vblasso(X,y, priors, I, thresh, init)
t(bX)%*%bX
t(bX)%*%bX+  diag(c(1/s2_0,mu_tau))
source("~/Documents/GitHub/BML_Trento_May2024/R/vblasso.R")
# Define prior parameters
priors = list(s=1, a_sigma = 2, b_sigma = 1, s2_0 = 10)
# Define initialization
init = list(mu_w = lm_prostate$coefficients, Sigma_w = diag(1,D+1),
a_sigma = priors$a_sigma, b_sigma = priors$b_sigma, mu_tau = rep(2*priors$s^2,D))
I = 100 #number of iterations
thresh = 0.001 # threshold for change in elbo
#Run MCMC
vblasso_out = vblasso(X,y, priors, I, thresh, init)
1/sqrt(s^2*(diag(Sigma_w)[-1] + mu_w[-1]^2)*a_sigma_vb/b_sigma_vb)
-1/2*a_sigma_v/b_sigma_vb
-1/2*a_sigma_vb/b_sigma_vb
sum(y^2)
- 2*t(y)%*%bX%*%mu_w
-1/2*a_sigma_vb/b_sigma_vb*(sum(y^2) - 2*t(y)%*%bX%*%mu_w +
sum((Sigma_w + mu_w%*%t(mu_w))*(t(bX)%*%bX+ diag(c(1/s2_0,mu_tau)))))
1/2*det(Sigma_w, logarithm = TRUE)
?det
det(Sigma_w, logarithm = TRUE)
determinant(Sigma_w, logarithm = TRUE)
determinant(Sigma_w, logarithm = TRUE)$nod
determinant(Sigma_w, logarithm = TRUE)$modulus
- D*log(s)
-1/s^2*sum(1/mu_tau)
- b_sigma/b_sigma_vb*a_simga_vb
- b_sigma/b_sigma_vb*a_sigma_vb
a_simga_vb*log(b_simga_vb)
a_sigma_vb*log(b_simga_vb)
a_sigma_vb*log(b_sigma_vb)
-1/2*a_sigma_vb/b_sigma_vb*(sum(y^2) - 2*t(y)%*%bX%*%mu_w +
sum((Sigma_w + mu_w%*%t(mu_w))*(t(bX)%*%bX+ diag(c(1/s2_0,mu_tau))))) +
1/2*determinant(Sigma_w, logarithm = TRUE)$modulus - D*log(s) -1/s^2*sum(1/mu_tau) - b_sigma/b_sigma_vb*a_sigma_vb -
a_sigma_vb*log(b_sigma_vb)
source("~/Documents/GitHub/BML_Trento_May2024/R/vblasso.R")
# Define prior parameters
priors = list(s=1, a_sigma = 2, b_sigma = 1, s2_0 = 10)
# Define initialization
init = list(mu_w = lm_prostate$coefficients, Sigma_w = diag(1,D+1),
a_sigma = priors$a_sigma, b_sigma = priors$b_sigma, mu_tau = rep(2*priors$s^2,D))
I = 100 #number of iterations
thresh = 0.001 # threshold for change in elbo
#Run MCMC
vblasso_out = vblasso(X,y, priors, I, thresh, init)
elbo
elbo
# Define prior parameters
priors = list(s=1, a_sigma = 2, b_sigma = 1, s2_0 = 10)
# Define initialization
init = list(mu_w = lm_prostate$coefficients, Sigma_w = diag(1,D+1),
a_sigma = priors$a_sigma, b_sigma = priors$b_sigma, mu_tau = rep(2*priors$s^2,D))
I = 100 #number of iterations
thresh = 0.001 # threshold for change in elbo
#Run MCMC
vblasso_out = vblasso(X,y, priors, I, thresh, init)
source("~/Documents/GitHub/BML_Trento_May2024/R/vblasso.R")
# Define prior parameters
priors = list(s=1, a_sigma = 2, b_sigma = 1, s2_0 = 10)
# Define initialization
init = list(mu_w = lm_prostate$coefficients, Sigma_w = diag(1,D+1),
a_sigma = priors$a_sigma, b_sigma = priors$b_sigma, mu_tau = rep(2*priors$s^2,D))
I = 100 #number of iterations
thresh = 0.001 # threshold for change in elbo
#Run MCMC
vblasso_out = vblasso(X,y, priors, I, thresh, init)
ggplot() +
geom_line(aes(x = c(1:length(vblasso_out$elbo), y = vblasso_out$elbo))) +
theme_bw()
vblasso_out$elbo
ggplot() +
geom_line(aes(x = c(1:length(vblasso_out$elbo)), y = vblasso_out$elbo)) +
theme_bw()
ggplot() +
geom_line(aes(x = c(1:length(vblasso_out$elbo)), y = vblasso_out$elbo)) +
theme_bw() +
labs( x ="epoch", y = "ELBO")
qnorm(0.975)
tidy_reg_conf_int$blasso = tidy_reg_conf_int$estimate
tidy_reg_conf_int$vblasso = vblasso_out$mu_w
tidy_reg_conf_int$conf.low = vblasso_out$mu_w+1.96*sqrt(vblasso_out$Sigma_w)
tidy_reg_conf_int$blasso = tidy_reg_conf_int$estimate
tidy_reg_conf_int$vblasso = vblasso_out$mu_w
tidy_reg_conf_int$conf.low = vblasso_out$mu_w+1.96*sqrt(diag(vblasso_out$Sigma_w))
tidy_reg_conf_int$conf.high = vblasso_out$mu_w-1.96*sqrt(diag(vblasso_out$Sigma_w))
tidy_reg_conf_int %>%
filter(term != "(Intercept)") %>%
# reorder the coefficients so that the largest is at the top of the plot
mutate(term = fct_reorder(term, estimate)) %>%
ggplot(aes(estimate, term)) +
geom_point() +
geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
# add lasso estimates
geom_point(aes(lasso, term),shape = 2, color = 'blue') +
geom_point(aes(blasso, term),shape = 3, color = 'green') +
geom_point(aes(ls, term),shape = 4, color = 'red') +
# add in a dotted line at zero
geom_vline(xintercept = 0, lty = 2) +
labs(
x = "Estimate of effect of variable on lpsa",
y = NULL,
title = "Coefficient plot with error bars") +
theme_bw()
vblasso_w = rmvnorm(dim(blasso_out$w)[1], vblasso_out$mu_w, vblasso_out$Sigma_w)
dim(vblasso_w)
# Compare scatter plot of samples from the two mcmc algorithms
vblasso_w = rmvnorm(dim(blasso_out$w)[1], vblasso_out$mu_w, vblasso_out$Sigma_w)
df = data.frame(w1 = c(blasso_out$w[,2], vblasso_w[,2]), w2 = c(blasso_out$w[,7], vblasso_w[,7]),
algorithm = c(rep("Gibbs", dim(blasso_out$w)[1]), rep("VB", dim(fit$beta)[2])))
ggplot(df) +
geom_point(aes(x = w1, y =w2, color = algorithm)) +
theme_bw()
?geom_histogram
names(X)
names(train_df)
coeff_names = names(train_df)[1:D]
coeff_names
# Compare scatter plot of samples from the two mcmc algorithms
coeff_names = names(train_df)[1:D]
ind1 = 1
ind2 = 6
vblasso_w = rmvnorm(dim(blasso_out$w)[1], vblasso_out$mu_w, vblasso_out$Sigma_w)
df = data.frame(w1 = c(blasso_out$w[,ind1 +1], vblasso_w[,ind1 +1]), w2 = c(blasso_out$w[,ind2 +1], vblasso_w[,ind2 +1]),
algorithm = c(rep("Gibbs", dim(blasso_out$w)[1]), rep("VB", dim(fit$beta)[2])))
ggplot(df) +
geom_point(aes(x = w1, y =w2, color = algorithm)) +
theme_bw() +
labs(x=coeff_names[ind1],y=coeff_names[ind2])
#Compare histogram of gibbs samples with the VB posterior
ind = 2
ggplot() +
geom_histogram(aes(blasso_out$w[,ind])) +
theme_bw() +
labs(x =coeff_names[ind])
# Compare scatter plot of samples from the two mcmc algorithms
coeff_names = names(train_df)[1:D]
ind1 = 1
ind2 = 6
vblasso_w = rmvnorm(dim(blasso_out$w)[1], vblasso_out$mu_w, vblasso_out$Sigma_w)
df = data.frame(w1 = c(blasso_out$w[,ind1 +1], vblasso_w[,ind1 +1]), w2 = c(blasso_out$w[,ind2 +1], vblasso_w[,ind2 +1]),
algorithm = c(rep("Gibbs", dim(blasso_out$w)[1]), rep("VB", dim(fit$beta)[2])))
ggplot(df) +
geom_point(aes(x = w1, y =w2, color = algorithm)) +
theme_bw() +
labs(x=coeff_names[ind1],y=coeff_names[ind2])
#Compare histogram of gibbs samples with the VB posterior
ind = 2
ggplot() +
geom_histogram(aes(x = blasso_out$w[,ind], y = ..density..), colour = 1, fill = "grey") +
theme_bw() +
labs(x =coeff_names[ind])
?dnorm
# Compare scatter plot of samples from the two mcmc algorithms
coeff_names = names(train_df)[1:D]
ind1 = 1
ind2 = 6
vblasso_w = rmvnorm(dim(blasso_out$w)[1], vblasso_out$mu_w, vblasso_out$Sigma_w)
df = data.frame(w1 = c(blasso_out$w[,ind1 +1], vblasso_w[,ind1 +1]), w2 = c(blasso_out$w[,ind2 +1], vblasso_w[,ind2 +1]),
algorithm = c(rep("Gibbs", dim(blasso_out$w)[1]), rep("VB", dim(fit$beta)[2])))
ggplot(df) +
geom_point(aes(x = w1, y =w2, color = algorithm)) +
theme_bw() +
labs(x=coeff_names[ind1],y=coeff_names[ind2])
#Compare histogram of gibbs samples with the VB posterior
ind = 2
xseq = seq(min(blasso_out$w[,ind])-.1,max(blasso_out$w[,ind])-.1,0.001)
ggplot() +
geom_histogram(aes(x = blasso_out$w[,ind], y = ..density..), colour = "green", fill = "grey") +
geom_line( aes( x = xseq, y = dnorm(xseq, mean = vblasso_out$mu_w[ind],sd = sqrt(vblasso_out$Sigma_w[ind,ind]))), colour = 1)
theme_bw() +
labs(x =coeff_names[ind])
# Compare scatter plot of samples from the two mcmc algorithms
coeff_names = names(train_df)[1:D]
ind1 = 1
ind2 = 6
vblasso_w = rmvnorm(dim(blasso_out$w)[1], vblasso_out$mu_w, vblasso_out$Sigma_w)
df = data.frame(w1 = c(blasso_out$w[,ind1 +1], vblasso_w[,ind1 +1]), w2 = c(blasso_out$w[,ind2 +1], vblasso_w[,ind2 +1]),
algorithm = c(rep("Gibbs", dim(blasso_out$w)[1]), rep("VB", dim(fit$beta)[2])))
ggplot(df) +
geom_point(aes(x = w1, y =w2, color = algorithm)) +
theme_bw() +
labs(x=coeff_names[ind1],y=coeff_names[ind2])
#Compare histogram of gibbs samples with the VB posterior
ind = 2
xseq = seq(min(blasso_out$w[,ind])-.1,max(blasso_out$w[,ind])-.1,0.01)
ggplot() +
geom_histogram(aes(x = blasso_out$w[,ind], y = ..density..), colour = "green", fill = "grey") +
geom_line( aes( x = xseq, y = dnorm(xseq, mean = vblasso_out$mu_w[ind],sd = sqrt(vblasso_out$Sigma_w[ind,ind]))), colour = 1) +
theme_bw() +
labs(x =coeff_names[ind])
# Compare scatter plot of samples from the two mcmc algorithms
coeff_names = names(train_df)[1:D]
ind1 = 1
ind2 = 6
vblasso_w = rmvnorm(dim(blasso_out$w)[1], vblasso_out$mu_w, vblasso_out$Sigma_w)
df = data.frame(w1 = c(blasso_out$w[,ind1 +1], vblasso_w[,ind1 +1]), w2 = c(blasso_out$w[,ind2 +1], vblasso_w[,ind2 +1]),
algorithm = c(rep("Gibbs", dim(blasso_out$w)[1]), rep("VB", dim(fit$beta)[2])))
ggplot(df) +
geom_point(aes(x = w1, y =w2, color = algorithm)) +
theme_bw() +
labs(x=coeff_names[ind1],y=coeff_names[ind2])
#Compare histogram of gibbs samples with the VB posterior
ind = 2
xseq = seq(min(blasso_out$w[,ind])-.1,max(blasso_out$w[,ind])-.1,0.01)
ggplot() +
geom_histogram(aes(x = blasso_out$w[,ind], y = ..density..), colour = "darkgreen", fill = "white") +
geom_line( aes( x = xseq, y = dnorm(xseq, mean = vblasso_out$mu_w[ind],sd = sqrt(vblasso_out$Sigma_w[ind,ind]))), colour = 1) +
theme_bw() +
labs(x =coeff_names[ind])
yhat = cbind(rep(1,Nnew),Xtest)%*%vblasso_out$mu_w
rmse_vblasso = sqrt(mean((yhat-test_df$lpsa)^2))
print(paste("The RMSE for the bayesian lasso is ",rmse_vblasso))
library(rstan)
install.packages('rstan')
library(rstan)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
util <- new.env()
source('stan_utility.R', local=util)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
writeLines(readLines("blasso.stan"))
writeLines(readLines(".././R/blasso.stan"))
writeLines(readLines(".././R/blasso.stan"))
writeLines(readLines(".././R/blasso.stan"))
writeLines(readLines(".././R/blasso.stan"))
writeLines(readLines(".././R/blasso.stan"))
writeLines(readLines(".././R/blasso.stan"))
inputdata <- list(X = X, N = N, y = y, D = D)
fit <- stan(model_code = ".././R/blasso.stan", data = inputdata, cores = 1, chains = 1, iter = 2000)
inputdata <- list(X = X, N = N, y = y, D = D)
fit <- stan(model_code = "blasso.stan", data = inputdata, cores = 1, chains = 1, iter = 2000)
inputdata <- list(X = X, N = N, y = y, D = D)
fit <- stan(file = ".././R/blasso.stan", data = inputdata, cores = 1, chains = 1, iter = 2000)
writeLines(readLines(".././R/blasso.stan"))
inputdata <- list(X = X, N = N, y = y, D = D)
fit <- stan(file = ".././R/blasso.stan", data = inputdata, cores = 1, chains = 1, iter = 2000)
writeLines(readLines(".././R/blasso.stan"))
inputdata <- list(X = X, N = N, y = y, D = D)
fit <- stan(file = ".././R/blasso.stan", data = inputdata, cores = 1, chains = 1, iter = 2000)
# we recommend running this is a fresh R session or restarting your current session
install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
library(cmdstanr)
set_cmdstan_path(path = NULL)
library(cmdstanr)
mod <- cmdstan_model(".././R/blasso.stan")
check_cmdstan_toolchain()
install_cmdstan(cores = 2)
cmdstan_path()
library(cmdstanr)
mod <- cmdstan_model(".././R/blasso.stan")
library(cmdstanr)
mod <- cmdstan_model(".././R/blasso.stan")
mod$print()
library(cmdstanr)
mod <- cmdstan_model(".././R/blasso.stan")
mod$print()
#writeLines(readLines(".././R/blasso.stan"))
inputdata <- list(X = t(X), N = N, y = y, D = D)
fit_mcmc <- mod$sample(
data = inputdata,
seed = 123,
chains = 1,
parallel_chains = 1
)
fit_mcmc$summary()
inputdata <- list(X = t(X), N = N, y = y, D = D)
fit_vb <- mod$variational(data = inputdata, algorithm="fullrank", seed = 123)
fit_vb$summary()
fit_sum = fit_vb$summary()
fit_sum
tidy_reg_conf_int
tidy_reg_conf_int$blasso = tidy_reg_conf_int$estimate
tidy_reg_conf_int$estimate = vblasso_out$mu_w
tidy_reg_conf_int$conf.low = vblasso_out$mu_w+1.96*sqrt(diag(vblasso_out$Sigma_w))
tidy_reg_conf_int$conf.high = vblasso_out$mu_w-1.96*sqrt(diag(vblasso_out$Sigma_w))
tidy_reg_conf_int %>%
filter(term != "(Intercept)") %>%
# reorder the coefficients so that the largest is at the top of the plot
mutate(term = fct_reorder(term, estimate)) %>%
ggplot(aes(estimate, term)) +
geom_point() +
geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
# add lasso estimates
geom_point(aes(lasso, term),shape = 2, color = 'blue') +
geom_point(aes(blasso, term),shape = 3, color = 'darkgreen') +
geom_point(aes(ls, term),shape = 4, color = 'red') +
# add in a dotted line at zero
geom_vline(xintercept = 0, lty = 2) +
labs(
x = "Estimate of effect of variable on lpsa",
y = NULL,
title = "Coefficient plot with error bars") +
theme_bw()
tidy_reg_conf_int$blasso = bl_what
tidy_reg_conf_int$estimate = vblasso_out$mu_w
tidy_reg_conf_int$conf.low = vblasso_out$mu_w+1.96*sqrt(diag(vblasso_out$Sigma_w))
tidy_reg_conf_int$conf.high = vblasso_out$mu_w-1.96*sqrt(diag(vblasso_out$Sigma_w))
tidy_reg_conf_int %>%
filter(term != "(Intercept)") %>%
# reorder the coefficients so that the largest is at the top of the plot
mutate(term = fct_reorder(term, estimate)) %>%
ggplot(aes(estimate, term)) +
geom_point(size = 2) +
geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
# add lasso estimates
geom_point(aes(lasso, term),shape = 2, color = 'blue', size = 2) +
geom_point(aes(blasso, term),shape = 3, color = 'darkgreen',size = 2) +
geom_point(aes(ls, term),shape = 4, color = 'red',size = 2) +
# add in a dotted line at zero
geom_vline(xintercept = 0, lty = 2) +
labs(
x = "Estimate of effect of variable on lpsa",
y = NULL,
title = "Coefficient plot with error bars") +
theme_bw()
fit_sum = fit_vb$summary()
fit_sum
tidy_reg_conf_int$vblasso = vblasso_out$mu_w
tidy_reg_conf_int$estimate = c(fit_sum$mean[D+3], fit_sum$mean[3:(D+2)])
tidy_reg_conf_int$conf.low = c(fit_sum$mean[D+3], fit_sum$mean[3:(D+2)])+1.96*c(fit_sum$sd[D+3], fit_sum$sd[3:(D+2)])
tidy_reg_conf_int$conf.high = c(fit_sum$mean[D+3], fit_sum$mean[3:(D+2)])-1.96*c(fit_sum$sd[D+3], fit_sum$sd[3:(D+2)])
tidy_reg_conf_int %>%
filter(term != "(Intercept)") %>%
# reorder the coefficients so that the largest is at the top of the plot
mutate(term = fct_reorder(term, estimate)) %>%
ggplot(aes(estimate, term)) +
geom_point() +
geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
# add lasso estimates
geom_point(aes(lasso, term),shape = 2, color = 'blue') +
geom_point(aes(blasso, term),shape = 3, color = 'darkgreen') +
geom_point(aes(vblasso, term),shape = 5, color = 'purple') +
geom_point(aes(ls, term),shape = 4, color = 'red') +
# add in a dotted line at zero
geom_vline(xintercept = 0, lty = 2) +
labs(
x = "Estimate of effect of variable on lpsa",
y = NULL,
title = "Coefficient plot with error bars") +
theme_bw()
vblasso_w2 = fit_vb$draw("w")
vblasso_w2 = fit_vb$draws("w")
vblasso_w2
# Compare scatter plot of samples from the two mcmc algorithms
coeff_names = names(train_df)[1:D]
ind1 = 1
ind2 = 6
vblasso_w = rmvnorm(dim(blasso_out$w)[1], vblasso_out$mu_w, vblasso_out$Sigma_w)
vblasso_w2 = fit_vb$draws("w")
df = data.frame(w1 = c(blasso_out$w[,ind1 +1], vblasso_w[,ind1 +1], vblasso_w2[ind1]),
w2 = c(blasso_out$w[,ind2 +1], vblasso_w[,ind2 +1], vblasso_w2[ind2]),
algorithm = c(rep("Gibbs", dim(blasso_out$w)[1]), rep("VB",dim(vblasso_w)[1]), rep("ADVI", dim(vblasso_w2)[1])))
# Compare scatter plot of samples from the two mcmc algorithms
coeff_names = names(train_df)[1:D]
ind1 = 1
ind2 = 6
vblasso_w = rmvnorm(dim(blasso_out$w)[1], vblasso_out$mu_w, vblasso_out$Sigma_w)
vblasso_w2 = fit_vb$draws("w")
df = data.frame(w1 = c(blasso_out$w[,ind1 +1], vblasso_w[,ind1 +1], vblasso_w2[,ind1]),
w2 = c(blasso_out$w[,ind2 +1], vblasso_w[,ind2 +1], vblasso_w2[,ind2]),
algorithm = c(rep("Gibbs", dim(blasso_out$w)[1]), rep("VB",dim(vblasso_w)[1]), rep("ADVI", dim(vblasso_w2)[1])))
ggplot(df) +
geom_point(aes(x = w1, y =w2, color = algorithm)) +
theme_bw() +
labs(x=coeff_names[ind1],y=coeff_names[ind2])
#Compare histogram of gibbs samples with the VB posterior
ind = 2
xseq = seq(min(blasso_out$w[,ind])-.1,max(blasso_out$w[,ind])-.1,0.01)
ggplot() +
geom_histogram(aes(x = blasso_out$w[,ind], y = ..density..), colour = "darkgreen", fill = "white") +
geom_histogram(aes(x = vblasso_w2[,ind], y = ..density..), colour = "purple", fill = "white") +
geom_line( aes( x = xseq, y = dnorm(xseq, mean = vblasso_out$mu_w[ind],sd = sqrt(vblasso_out$Sigma_w[ind,ind]))), colour = 1) +
theme_bw() +
labs(x =coeff_names[ind])
# Compare scatter plot of samples from the two mcmc algorithms
coeff_names = names(train_df)[1:D]
ind1 = 1
ind2 = 6
vblasso_w = rmvnorm(dim(blasso_out$w)[1], vblasso_out$mu_w, vblasso_out$Sigma_w)
vblasso_w2 = fit_vb$draws("w")
df = data.frame(w1 = c(blasso_out$w[,ind1 +1], vblasso_w[,ind1 +1], vblasso_w2[,ind1]),
w2 = c(blasso_out$w[,ind2 +1], vblasso_w[,ind2 +1], vblasso_w2[,ind2]),
algorithm = c(rep("Gibbs", dim(blasso_out$w)[1]), rep("VB",dim(vblasso_w)[1]), rep("ADVI", dim(vblasso_w2)[1])))
ggplot(df) +
geom_point(aes(x = w1, y =w2, color = algorithm)) +
theme_bw() +
labs(x=coeff_names[ind1],y=coeff_names[ind2])
#Compare histogram of gibbs samples with the VB posterior
ind = 2
xseq = seq(min(blasso_out$w[,ind])-.1,max(blasso_out$w[,ind])-.1,0.01)
ggplot() +
geom_histogram(aes(x = blasso_out$w[,ind], y = ..density..), colour = "darkgreen", fill = "white") +
geom_histogram(aes(x = vblasso_w2[,ind-1, y = ..density..), colour = "purple", fill = "white") +
# Compare scatter plot of samples from the two mcmc algorithms
coeff_names = names(train_df)[1:D]
ind1 = 1
ind2 = 6
vblasso_w = rmvnorm(dim(blasso_out$w)[1], vblasso_out$mu_w, vblasso_out$Sigma_w)
vblasso_w2 = fit_vb$draws("w")
df = data.frame(w1 = c(blasso_out$w[,ind1 +1], vblasso_w[,ind1 +1], vblasso_w2[,ind1]),
w2 = c(blasso_out$w[,ind2 +1], vblasso_w[,ind2 +1], vblasso_w2[,ind2]),
algorithm = c(rep("Gibbs", dim(blasso_out$w)[1]), rep("VB",dim(vblasso_w)[1]), rep("ADVI", dim(vblasso_w2)[1])))
ggplot(df) +
geom_point(aes(x = w1, y =w2, color = algorithm)) +
theme_bw() +
labs(x=coeff_names[ind1],y=coeff_names[ind2])
#Compare histogram of gibbs samples with the VB posterior
ind = 2
xseq = seq(min(blasso_out$w[,ind])-.1,max(blasso_out$w[,ind])-.1,0.01)
ggplot() +
geom_histogram(aes(x = blasso_out$w[,ind], y = ..density..), colour = "darkgreen", fill = "white") +
geom_histogram(aes(x = vblasso_w2[,ind-1], y = ..density..), colour = "purple", fill = "white") +
geom_line( aes( x = xseq, y = dnorm(xseq, mean = vblasso_out$mu_w[ind],sd = sqrt(vblasso_out$Sigma_w[ind,ind]))), colour = 1) +
theme_bw() +
labs(x =coeff_names[ind])
glmnet.out=glmnet(X, y, alpha=1, lambda = bestlam, standardize = FALSE)
glmnet.out$beta
tidy_reg_conf_int$lasso = as.matrix(predict(glmnet.out,type="coefficients",s=bestlam))
