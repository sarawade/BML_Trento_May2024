---
title: "Bayesian Shirnkage"
author: "Sara Wade"
date: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggpubr)
library(gridExtra)
library(GGally)
library(broom)
library(glmnet)
library(statmod)
library(mvtnorm)
library(coda)

source(".././R/gibbs_blasso.R")
```

## Data

The data for comes from the Elements of Statistical Learning textbook; and is originally from a study by Stamey et al. (1989) in which they examined the relationship between the level of prostate-specific antigen (psa) and a number of clinical measures in men who were about to receive a prostatectomy. The variables are as follows:

- lpsa - log of the level of prostate-specific antigen
- lcavol - log cancer volume
- lweight - log prostate weight
- age - patient age
- lbph - log of the amount of benign prostatic hyperplasia
- svi - seminal vesicle invasion
- lcp - log of capsular penetration
- gleason - Gleason score
- pgg45 - percent of Gleason scores 4 or 5
- train - test / train split used in ESL

These data are available in `prostate.csv`. Let's start by reading in the data and some basic EDA.

```{r data}
prostate = read.csv(".././data/prostate.csv")
head(prostate)
```

```{r datasum}
summary(prostate)
```

```{r datavis, warning=FALSE}
# Visualize the data (except the last column indicating the test/train split)
prostate_gg = prostate
prostate_gg$svi = as.factor(prostate_gg$svi)
ggpairs(prostate_gg, columns = 1:(ncol(prostate)-1))
```

There are  $N=97$ observations with no missingness. We observe that:

- The variables `age` and `ppg45` are integer valued, with latter representing a percent and restricted to the range  $[0,100]$. 
- The variable `svi` is binary and the variable `gleason` is an ordered categorical variable. 
-The other variables are numerical.

We see that both `lcavol` and `lcp` seem to have a linear relationship with the response `lpsa`.

### Split the data
```{r datae}
# Split the data
train_df = data.frame(prostate[prostate$train==1,])
train_df = select(train_df, !c(train))
test_df = data.frame(prostate[prostate$train==0,])
test_df = select(test_df, !c(train))


N = dim(train_df)[1] #number of data points
D = dim(train_df) [2] -1 #number of features
Nnew = dim(test_df)[1]
print(paste("Number of data points =",N, " and number of features =", D))
```

### Fit linear regression

Let's start by fitting a baseline linear regression model.

First, we will scale the inputs.

```{r scale}
# Scale the data
train.m = apply(train_df, 2, mean)
train.s = apply(train_df,2,sd)
train_df = (train_df - matrix(train.m,N,D+1,byrow = TRUE))/matrix(train.s,N,D+1,byrow = TRUE)
test_df = (test_df - matrix(train.m,Nnew,D+1,byrow = TRUE))/matrix(train.s,Nnew,D+1,byrow = TRUE)
```

Fit the linear regression model and visualize the coefficients.

```{r lm}
# Fit lm
lm_prostate = lm(lpsa~., data=train_df)
summary(lm_prostate)
```
```{r lmcoef}
tidy_reg_conf_int = tidy(lm_prostate,conf.int =TRUE)
tidy_reg_conf_int %>%
  filter(term != "(Intercept)") %>%
  # reorder the coefficients so that the largest is at the top of the plot
  mutate(term = fct_reorder(term, estimate)) %>%
  ggplot(aes(estimate, term)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  # add in a dotted line at zero
  geom_vline(xintercept = 0, lty = 2) +
  labs(
    x = "Estimate of effect of variable on lpsa",
    y = NULL,
    title = "Coefficient plot with error bars") +
  theme_bw()
```

Compute the RMSE on the test data 

```{r lmrmse}
yhat = predict(lm_prostate, test_df)
rmse_lm = sqrt(mean((yhat-test_df$lpsa)^2))
print(paste("The RMSE for the linear model is ",rmse_lm))
```

## Lasso

Now, we will use `glmnet` to fit a lasso model. First, we need to use cross-validation to tune $\lambda$. 

```{r lassocv}
set.seed (1)
X = as.matrix(train_df[,1:D])
y = train_df$lpsa
cv.out=cv.glmnet(X, y, alpha=1, standardize = FALSE)
plot(cv.out)
```

```{r bestlam}
bestlam=cv.out$lambda.min
print(bestlam)
```


Now, let's refit with the best lambda

```{r lasso}
glmnet.out=glmnet(X, y, alpha=1, lambda = bestlam, standardize = FALSE)
glmnet.out$beta
```
Add the lasso estimates to our plot:

```{r lassocoef}
tidy_reg_conf_int = tidy(lm_prostate,conf.int =TRUE)
tidy_reg_conf_int$lasso = as.matrix(predict(glmnet.out,type="coefficients",s=bestlam))
tidy_reg_conf_int %>%
  filter(term != "(Intercept)") %>%
  # reorder the coefficients so that the largest is at the top of the plot
  mutate(term = fct_reorder(term, estimate)) %>%
  ggplot(aes(estimate, term)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  # add lasso estimates
  geom_point(aes(lasso, term),shape = 2, color = 'blue') +
  # add in a dotted line at zero
  geom_vline(xintercept = 0, lty = 2) +
  labs(
    x = "Estimate of effect of variable on lpsa",
    y = NULL,
    title = "Coefficient plot with error bars") +
  theme_bw()
```


Compute the RMSE on the test data 

```{r lassormse}
Xtest = as.matrix(test_df[,1:D])
yhat = predict(glmnet.out, s = bestlam, Xtest)
rmse_lasso = sqrt(mean((yhat-test_df$lpsa)^2))
print(paste("The RMSE for the lasso is ",rmse_lasso))
```

## Ridge and Elastic Net

Exercise: Try to repeat for ridge and elastic net and compare the coefficients and RMSE.

## Bayesian Lasso

Run a simple Gibbs sampling algorithm.

```{r blassosetup}

# Define prior parameters
priors = list(s=1, a_sigma = 2, b_sigma = 1, s2_0 = 10)

I = 2000 #number of iterations

# Initialize
w_init = lm_prostate$coefficients
sigma_init = mean(lm_prostate$residuals^2)

#Run MCMC
blasso_out = gibbs_blasso(X,y, priors, I, w_init, sigma_init)
```

Check mixing and determine burnin (note here we started with good initial values!)

```{r blasso_mcmc}
# Traceplots
traceplot(as.mcmc(blasso_out$w))

traceplot(as.mcmc(blasso_out$tau))

traceplot(as.mcmc(blasso_out$sigma))

# Check other diagnostics in coda

# Remove burnin
b = 1000
blasso_out$w = blasso_out$w[b:I,]
blasso_out$tau = blasso_out$tau[b:I,]
blasso_out$sigma = blasso_out$sigma[b:I]
```

Let's compare the coefficents

```{r blassocoef}
# Compute posterior mean and HPD intervals
bl_what = apply(blasso_out$w,2,mean)
bl_wci = apply(blasso_out$w,2,function(x){HPDinterval(as.mcmc(x))})
  
tidy_reg_conf_int$ls = tidy_reg_conf_int$estimate 
tidy_reg_conf_int$estimate = bl_what
tidy_reg_conf_int$conf.low = bl_wci[1,]
tidy_reg_conf_int$conf.high = bl_wci[2,]

tidy_reg_conf_int %>%
  filter(term != "(Intercept)") %>%
  # reorder the coefficients so that the largest is at the top of the plot
  mutate(term = fct_reorder(term, estimate)) %>%
  ggplot(aes(estimate, term)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  # add lasso estimates
  geom_point(aes(lasso, term),shape = 2, color = 'blue') +
  geom_point(aes(ls, term),shape = 4, color = 'red') +
  # add in a dotted line at zero
  geom_vline(xintercept = 0, lty = 2) +
  labs(
    x = "Estimate of effect of variable on lpsa",
    y = NULL,
    title = "Coefficient plot with error bars") +
  theme_bw()
```

Compute the RMSE:

```{r blassormse}
 
yhat = cbind(rep(1,Nnew),Xtest)%*%bl_what
rmse_blasso = sqrt(mean((yhat-test_df$lpsa)^2))
print(paste("The RMSE for the bayesian lasso is ",rmse_blasso))
```

## Exercises

1. Try changing s and seeing how it affects the results. What is a reasonable value based on the estimated $\lambda$ from lasso?
2. Try changing the intialization to a random draw from the prior. How does this affect?
3. Try adding a step to sample $s$.

## Bayesreg

You can also try to compare with the package [`bayesreg`](https://rdrr.io/cran/bayesreg/man/bayesreg-package.html), which implements a number of shrinkage priors.

```{r bayesreg}
library(bayesreg)
fit <- bayesreg(lpsa~., train_df, model = "gaussian", prior = "lasso", n.samples = 2000)
```

How do the results compare?

```{r blasso_mcmc2}
# Define prior parameters
traceplot(as.mcmc(t(fit$beta)))

traceplot(as.mcmc(t(fit$beta0)))

traceplot(as.mcmc(t(fit$sigma2)))

traceplot(as.mcmc(t(fit$tau2)))

# Check other diagnostics in coda

# Remove burnin
b = 1000
fit$beta = fit$beta[,b:I]
fit$beta0 = fit$beta0[b:I]
fit$sigma2 = fit$sigma2[b:I]
fit$tau2 = fit$tau2 [b:I]
```


```{r blasso_mcmc_comp}
# Compare scatter plot of samples from the two mcmc algorithms
coeff_names = names(train_df)[1:D]
ind1 = 1
ind2 = 6
df = data.frame(w1 = c(blasso_out$w[,ind1 +1], fit$beta[ind1,]), w2 = c(blasso_out$w[,ind2 +1], fit$beta[ind2,]), 
                mcmc = c(rep("Gibbs", dim(blasso_out$w)[1]), rep("HMC", dim(fit$beta)[2])))
ggplot(df) +
  geom_point(aes(x = w1, y =w2, color = mcmc)) + 
  theme_bw() +
  labs(x=coeff_names[ind1],y=coeff_names[ind2])
```