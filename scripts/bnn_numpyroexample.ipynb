{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqmrypXDEqK6"
      },
      "source": [
        "# Bayesian neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4qn0nvhEqK8"
      },
      "source": [
        "We will consider fitting a Bayesian neural network to some toy data using `numpyro`.\n",
        "\n",
        "Let's start by loading the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58vSdHjEEqK9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "!pip install numpyro\n",
        "import numpyro\n",
        "import numpyro.optim as optim\n",
        "from numpyro import deterministic\n",
        "\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS, SVI, Predictive, Trace_ELBO\n",
        "from numpyro.infer.autoguide import AutoNormal, AutoMultivariateNormal, AutoDelta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFWEQOp-EqK-"
      },
      "source": [
        "## Toy data\n",
        "\n",
        "We start by generating data in a simple one-dimensional input and output setting. The inputs are generated from a uniform distribution, and the outputs are generated as noisy observation of the function $y=(x+2)^2(x/1-1)$ or $y = x^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R1dO30cPEqK_"
      },
      "outputs": [],
      "source": [
        "def function_true2(input):\n",
        "  return jnp.power(input +2, 2.0)*(input/2-1)\n",
        "\n",
        "def function_true(input):\n",
        "  return jnp.power(input, 2.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI6865g_EqK_"
      },
      "outputs": [],
      "source": [
        "N = 500 # sample size\n",
        "\n",
        "# Generate inputs\n",
        "X_train = random.uniform(random.PRNGKey(0), shape=(N,),  minval=-4, maxval=4)\n",
        "\n",
        "# Generate outputs\n",
        "sigma = 1\n",
        "m_Y = function_true2(X_train)\n",
        "# or m_Y = function_true2(X\n",
        "y_train =  m_Y + sigma*random.normal(random.PRNGKey(1), shape=(N,))\n",
        "\n",
        "# Plot the data\n",
        "x_idx = jnp.argsort(X_train)\n",
        "fig = plt.figure(figsize=(4, 3))\n",
        "plt.plot(X_train[x_idx], m_Y[x_idx],  label = 'True function')\n",
        "plt.scatter(X_train, y_train, marker = '.', linewidths = 0.1, label = 'Data')\n",
        "plt.fill_between(X_train[x_idx], (m_Y - 2*sigma)[x_idx], (m_Y + 2*sigma)[x_idx], alpha = 0.2, label = r'true $\\pm 2 \\sigma$')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elFsvaTgEqLA"
      },
      "source": [
        "Let's generate the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfOFphm2EqLA"
      },
      "outputs": [],
      "source": [
        "N_test = 400 # sample size\n",
        "\n",
        "# Generate test inputs\n",
        "X_test = random.uniform(random.PRNGKey(0), shape=(N_test,),  minval=-4, maxval=4)\n",
        "\n",
        "# Generate test outputs\n",
        "m_Yt = function_true2(X_test)\n",
        "# or m_Y = function_true2(X\n",
        "y_test =  m_Yt + sigma*random.normal(random.PRNGKey(1), shape=(N_test,))\n",
        "\n",
        "\n",
        "# Plot the training and test data together\n",
        "fig, axs = plt.subplots(2,1,figsize=(4, 5), layout=\"constrained\")\n",
        "\n",
        "axs[0].scatter(x=X_train, y=y_train,  marker = '.', linewidths = 0.1, label=\"train\")\n",
        "axs[0].legend(loc=\"upper left\")\n",
        "axs[0].set(xlabel=\"x\", ylabel=\"y\")\n",
        "axs[1].scatter(x=X_test, y=y_test,  marker = '.', linewidths = 0.1, label=\"test\")\n",
        "axs[1].legend(loc=\"upper left\")\n",
        "axs[1].set(xlabel=\"x\", ylabel=\"y\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lttjfOXkEqLA"
      },
      "source": [
        "Let's rescale the data before fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8JreIb2EqLB"
      },
      "outputs": [],
      "source": [
        "x_scaler = StandardScaler()\n",
        "x_train_scaled = x_scaler.fit_transform(X_train[:, None])\n",
        "x_train_scaled = jnp.array(x_train_scaled)\n",
        "x_test_scaled = x_scaler.transform(X_test[:, None])\n",
        "x_test_scaled = jnp.array(x_test_scaled)\n",
        "\n",
        "fig, axs = plt.subplots(2,1,figsize=(4, 5), layout=\"constrained\")\n",
        "\n",
        "axs[0].scatter(x=x_train_scaled, y=y_train,  marker = '.', linewidths = 0.1, label=\"train\")\n",
        "axs[0].legend(loc=\"upper left\")\n",
        "axs[0].set(xlabel=\"x scaled\", ylabel=\"y\")\n",
        "axs[1].scatter(x=x_test_scaled, y=y_test,  marker = '.', linewidths = 0.1, label=\"test\")\n",
        "axs[1].legend(loc=\"upper left\")\n",
        "axs[1].set(xlabel=\"x scaled\", ylabel=\"y\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HknrTTWuEqLB"
      },
      "source": [
        "### BNN model\n",
        "\n",
        "Let's build a simple BNN following this [paper](https://jmhl.org/wp-content/uploads/2015/05/pbp-icml2015.pdf), with code mostly from [numpyro documentation](https://num.pyro.ai/en/stable/examples/stein_bnn.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VYzgfZSXEqLB"
      },
      "outputs": [],
      "source": [
        "def model(x, y=None, hidden_dim=10):\n",
        "\n",
        "\n",
        "    prec_nn = numpyro.sample(\n",
        "        \"prec_nn\", dist.Gamma(6.0, 6.0)\n",
        "    )  # hyper prior for precision of nn weights and biases\n",
        "\n",
        "    n, m = x.shape\n",
        "\n",
        "    with numpyro.plate(\"l1_hidden\", hidden_dim, dim=-1):\n",
        "        # prior l=1 bias term\n",
        "        b1 = numpyro.sample(\n",
        "            \"nn_b1\",\n",
        "            dist.Normal(\n",
        "                0.0,\n",
        "                1.0 / jnp.sqrt(prec_nn*(m+1)),\n",
        "            ),\n",
        "        )\n",
        "        assert b1.shape == (hidden_dim,)\n",
        "\n",
        "        with numpyro.plate(\"l1_feat\", m, dim=-2):\n",
        "            w1 = numpyro.sample(\n",
        "                \"nn_w1\", dist.Normal(0.0, 1.0 / jnp.sqrt(prec_nn*(m+1)))\n",
        "            )  # prior on l=1 weights\n",
        "            assert w1.shape == (m, hidden_dim)\n",
        "\n",
        "    with numpyro.plate(\"l2_hidden\", hidden_dim, dim=-1):\n",
        "        w2 = numpyro.sample(\n",
        "            \"nn_w2\", dist.Normal(0.0, 1.0 / jnp.sqrt(prec_nn*(hidden_dim+1)))\n",
        "        )  # prior on output weights\n",
        "\n",
        "    b2 = numpyro.sample(\n",
        "        \"nn_b2\", dist.Normal(0.0, 1.0 / jnp.sqrt(prec_nn*(hidden_dim+1)))\n",
        "    )  # prior on output bias term\n",
        "\n",
        "    # precision prior on observations\n",
        "    prec_obs = numpyro.sample(\"prec_obs\", dist.Gamma(6, 6))\n",
        "\n",
        "    loc_y = deterministic(\"y_pred\", jnp.maximum(x @ w1 + b1, 0) @ w2 + b2)\n",
        "\n",
        "    with numpyro.plate( \"data\", x.shape[0], dim=-1):\n",
        "\n",
        "        numpyro.sample( \"y\",\n",
        "            dist.Normal(loc_y, 1.0 / jnp.sqrt(prec_obs)),  obs=y,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d0qYZjIEqLC"
      },
      "outputs": [],
      "source": [
        "numpyro.render_model(model, model_args=(x_train_scaled, y_train, 20),\n",
        "    render_distributions=True,\n",
        "    render_params=True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxxyCW8kEqLC"
      },
      "source": [
        "## SVI\n",
        "\n",
        "Now, we will use SVI to approximate the posterior. Note that the variational posterior is called `guide`. For details, see http://pyro.ai/examples/svi_part_i.html. Here we are assuming a (diagonal) normal using `AutoNormal`. For other automatic guide options, see https://num.pyro.ai/en/stable/autoguide.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Y8gf2Ww-EqLC"
      },
      "outputs": [],
      "source": [
        "numpyro.set_platform('cpu')\n",
        "rng_key, rng_key_predict = random.split(random.PRNGKey(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "weYceQvJEqLC"
      },
      "outputs": [],
      "source": [
        "def do_SVI(model, rng_key, numsteps, x, y, hidden_dim):\n",
        "    guide = AutoNormal(model)\n",
        "    optimizer = optim.Adam(0.01)\n",
        "    svi = SVI(model, guide, optimizer, Trace_ELBO())\n",
        "    svi_results = svi.run(rng_key = rng_key, num_steps = numsteps, x=x, y = y, hidden_dim = hidden_dim)\n",
        "    params = svi_results.params\n",
        "    losses = svi_results.losses\n",
        "    return losses, params, guide\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NaZyxszsEqLD"
      },
      "outputs": [],
      "source": [
        "D_h = 20\n",
        "numsteps= 1000\n",
        "num_samples=2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mot-DmKVEqLD"
      },
      "outputs": [],
      "source": [
        "svi_losses, svi_params, svi_guide = do_SVI(model=model, rng_key=rng_key, numsteps=numsteps, x = x_train_scaled, y = y_train, hidden_dim=D_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VsSCfEX0EqLD"
      },
      "outputs": [],
      "source": [
        "posterior_predictive = Predictive(model=model, guide=svi_guide, params=svi_params, num_samples=num_samples)\n",
        "svi_predictions = posterior_predictive(rng_key = rng_key_predict, x=x_test_scaled, y=None, hidden_dim=D_h)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see visualize the predictions and uncertainty."
      ],
      "metadata": {
        "id": "n4OuMC6XM-BT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_g30scyEqLD"
      },
      "outputs": [],
      "source": [
        "# Extract the mean and standard deviation for the predictions on the test data\n",
        "mean_pred = jnp.mean(svi_predictions['y_pred'], axis = 0)\n",
        "std_pred = jnp.std(svi_predictions['y_pred'], axis = 0)\n",
        "x_idx = jnp.argsort(x_test_scaled[:, 0])\n",
        "\n",
        "# make plots\n",
        "fig, ax = plt.subplots(1, figsize=(6, 6), constrained_layout=True)\n",
        "\n",
        "# plot training data\n",
        "ax.plot(x_test_scaled[:, 0], y_test, '.', label = 'Samples for testing')\n",
        "# plot mean prediction\n",
        "ax.plot(x_test_scaled[:, 0][x_idx], mean_pred[x_idx], color = \"purple\", ls=\"solid\", lw=2.0, label = 'Prediction mean')\n",
        "# plot true mean\n",
        "ax.plot(x_test_scaled[:, 0][x_idx], m_Yt[x_idx], color = \"red\", ls=\"solid\", lw=2.0, label = 'True mean')\n",
        "#plot uncertainty\n",
        "ax.fill_between(x_test_scaled[:, 0][x_idx], mean_pred[x_idx] -2*std_pred[x_idx], mean_pred[x_idx] + 2*std_pred[x_idx], alpha = 0.2, label = 'Uncertainty')\n",
        "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"SVI\")\n",
        "fig.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Flvor2oEqLE"
      },
      "outputs": [],
      "source": [
        "plt.plot(jnp.arange(len(svi_losses)), -svi_losses, linewidth = 0.5, label = 'ELBO')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the predictive accuracy and the evaluate the uncertainty by counting the number of points contined within the credible interval for different levels"
      ],
      "metadata": {
        "id": "JanKCQmIOm7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE and R2\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "print('MSE for the outputs is ', mean_squared_error(y_test,mean_pred))\n",
        "print('MSE for the function is ', mean_squared_error(m_Yt,mean_pred))\n",
        "print('R2 for the outputs is ', r2_score(y_test,mean_pred))"
      ],
      "metadata": {
        "id": "Tz8l7JzTOlJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = np.arange(0.5,1,0.01)\n",
        "ec = np.zeros(q.shape)\n",
        "\n",
        "for i in range(q.shape[0]):\n",
        "  # Compute credible interval\n",
        "  lower_pred = np.quantile(svi_predictions['y_pred'], q = (1-q[i])/2, axis = 0)\n",
        "  upper_pred = np.quantile(svi_predictions['y_pred'], q = q[i]+ (1-q[i])/2, axis = 0)\n",
        "  # Summarize coverage\n",
        "  ec[i]= np.sum((m_Yt>lower_pred)&(m_Yt<upper_pred))/N_test\n",
        "\n",
        "plt.plot(q, ec, linewidth = 2, label = 'Empirical Coverage')\n",
        "plt.plot(q,q, linestyle = 'dashed', linewidth = 2)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MFKwb702Y4d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "1. See how the results change by altering different settings:\n",
        "  *   increasing the number of hidden units\n",
        "  *   change the prior to Laplace (`dist.Laplace`)\n",
        "  *   change the hyperparameters of the Gamma priors.\n",
        "2. Try changing the variational posterior, e.g. to `AutoMultivariateNormal` to remove the mean-field assumption.\n",
        "3. Try adding another layer"
      ],
      "metadata": {
        "id": "dwKSIS7eiM2q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ_kAzlmEqLE"
      },
      "source": [
        "## MCMC\n",
        "\n",
        "Now, let's compare with MCMC. Here we will use the NUTS (no-U-turn sampler). For other MCMC algorithms and options, see https://num.pyro.ai/en/stable/mcmc.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "C9R95lZeEqLE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_mcmc(model, num_chains, num_samples, num_warmup, rng_key, x, y, hidden_dim):\n",
        "    start = time.time()\n",
        "    kernel = NUTS(model)\n",
        "    mcmc = MCMC(\n",
        "        kernel,\n",
        "        num_warmup=num_warmup,\n",
        "        num_samples=num_samples,\n",
        "        num_chains=num_chains,\n",
        "        progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n",
        "    )\n",
        "    mcmc.run(rng_key = rng_key, x=x, y=y, hidden_dim = hidden_dim)\n",
        "    # mcmc.print_summary()\n",
        "    print(\"\\nMCMC  time:\", time.time() - start)\n",
        "    return mcmc.get_samples()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0g_wG6sREqLF"
      },
      "outputs": [],
      "source": [
        "num_warmup=2000\n",
        "num_samples=5000\n",
        "num_chains=1\n",
        "numpyro.set_host_device_count(num_chains)\n",
        "D_h = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugDn2TslEqLF"
      },
      "outputs": [],
      "source": [
        "posterior_samples = run_mcmc(model = model,  num_chains = num_chains,num_samples = num_samples, num_warmup = num_warmup, rng_key = rng_key, x = x_train_scaled, y = y_train, hidden_dim=D_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3azfA8oUEqLF"
      },
      "outputs": [],
      "source": [
        "predictive = Predictive(model = model, posterior_samples=posterior_samples)\n",
        "predictions = predictive(rng_key = rng_key_predict, x=x_test_scaled, y = None, hidden_dim = D_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfpahS3fEqLF"
      },
      "outputs": [],
      "source": [
        "\n",
        "mean_pred = jnp.mean(predictions['y_pred'], axis = 0)\n",
        "lower_pred = jnp.quantile(predictions['y_pred'], q = 0.0275, axis = 0)\n",
        "upper_pred = jnp.quantile(predictions['y_pred'], q = 0.975, axis = 0)\n",
        "x_idx = jnp.argsort(x_test_scaled[:, 0])\n",
        "\n",
        "# make plots\n",
        "fig, ax = plt.subplots(1, figsize=(6, 6), constrained_layout=True)\n",
        "\n",
        "# plot training data\n",
        "ax.plot(x_test_scaled[:, 0], y_test, '.', label = 'Samples for testing')\n",
        "# plot mean prediction\n",
        "# ax.plot(x_test_scaled[:, 0], mean_pred[:, 0], '.')\n",
        "ax.plot(x_test_scaled[:, 0][x_idx], mean_pred[x_idx], color = \"purple\", ls=\"solid\", lw=2.0, label = 'Prediction mean')\n",
        "# plot true mean\n",
        "ax.plot(x_test_scaled[:, 0][x_idx], m_Yt[x_idx], color = \"red\", ls=\"solid\", lw=2.0, label = 'True mean')\n",
        "#plot uncertainty\n",
        "ax.fill_between(x_test_scaled[:, 0][x_idx], lower_pred[x_idx], upper_pred[x_idx], alpha = 0.2, label = 'Uncertainty')\n",
        "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"MCMC\")\n",
        "fig.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the predictive accuracy and uncertainty with VB."
      ],
      "metadata": {
        "id": "IiEQ-S5HKsBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE and R2\n",
        "print('MSE for the outputs is ', mean_squared_error(y_test,mean_pred))\n",
        "print('MSE for the function is ', mean_squared_error(m_Yt,mean_pred))\n",
        "print('R2 for the outputs is ', r2_score(y_test,mean_pred))"
      ],
      "metadata": {
        "id": "WgiAwO0PK1Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = np.arange(0.5,1,0.01)\n",
        "ec_mcmc = np.zeros(q.shape)\n",
        "\n",
        "for i in range(q.shape[0]):\n",
        "  # Compute credible interval\n",
        "  lower_pred = np.quantile(predictions['y_pred'], q = (1-q[i])/2, axis = 0)\n",
        "  upper_pred = np.quantile(predictions['y_pred'], q = q[i]+ (1-q[i])/2, axis = 0)\n",
        "  # Summarize coverage\n",
        "  ec_mcmc[i]= np.sum((m_Yt>lower_pred)&(m_Yt<upper_pred))/N_test\n",
        "\n",
        "plt.plot(q, ec, linewidth = 2, label = 'VB')\n",
        "plt.plot(q,q, linestyle = 'dashed', linewidth = 2)\n",
        "plt.plot(q, ec_mcmc, linewidth = 2, label = 'MCMC')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YBlzKWjCLPe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAP\n",
        "\n",
        "Now, let's try MAP estimation by changing the variational posterior to AutoDelta"
      ],
      "metadata": {
        "id": "aluN291GZ_p1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_MAP(model, rng_key, numsteps, x, y, hidden_dim):\n",
        "    guide = AutoDelta(model)\n",
        "    optimizer = optim.Adam(0.01)\n",
        "    svi = SVI(model, guide, optimizer, Trace_ELBO())\n",
        "    svi_results = svi.run(rng_key = rng_key, num_steps = numsteps, x=x, y = y, hidden_dim = hidden_dim)\n",
        "    params = svi_results.params\n",
        "    losses = svi_results.losses\n",
        "    return losses, params, guide"
      ],
      "metadata": {
        "id": "xpeZnwf4al-F"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D_h = 20\n",
        "numsteps= 1000\n",
        "num_samples=2000"
      ],
      "metadata": {
        "id": "gQ1nBdokbKBy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_losses, map_params, map_guide = do_MAP(model=model, rng_key=rng_key, numsteps=numsteps, x = x_train_scaled, y = y_train, hidden_dim=D_h)"
      ],
      "metadata": {
        "id": "TQ5eojyhbK8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_predictive = Predictive(model=model, guide=map_guide, params=map_params, num_samples=num_samples)\n",
        "map_predictions = posterior_predictive(rng_key = rng_key_predict, x=x_test_scaled, y=None, hidden_dim=D_h)"
      ],
      "metadata": {
        "id": "BYCS6lY4bfMM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_pred = jnp.mean(map_predictions['y_pred'], axis = 0)\n",
        "lower_pred = jnp.quantile(map_predictions['y'], q = 0.0275, axis = 0)\n",
        "upper_pred = jnp.quantile(map_predictions['y'], q = 0.975, axis = 0)\n",
        "x_idx = jnp.argsort(x_test_scaled[:, 0])\n",
        "\n",
        "# make plots\n",
        "fig, ax = plt.subplots(1, figsize=(6, 6), constrained_layout=True)\n",
        "\n",
        "# plot training data\n",
        "ax.plot(x_test_scaled[:, 0], y_test, '.', label = 'Samples for testing')\n",
        "# plot mean prediction\n",
        "# ax.plot(x_test_scaled[:, 0], mean_pred[:, 0], '.')\n",
        "ax.plot(x_test_scaled[:, 0][x_idx], mean_pred[x_idx], color = \"purple\", ls=\"solid\", lw=2.0, label = 'Prediction mean')\n",
        "# plot true mean\n",
        "ax.plot(x_test_scaled[:, 0][x_idx], m_Yt[x_idx], color = \"red\", ls=\"solid\", lw=2.0, label = 'True mean')\n",
        "#plot uncertainty\n",
        "ax.fill_between(x_test_scaled[:, 0][x_idx], lower_pred[x_idx], upper_pred[x_idx], alpha = 0.2, label = 'Uncertainty')\n",
        "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"MAP\")\n",
        "fig.legend()\n"
      ],
      "metadata": {
        "id": "P2ylykQmdltQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's add some additional out of sample test points and see what happens."
      ],
      "metadata": {
        "id": "5HO3vJdDgDrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_test2 = 500 # sample size\n",
        "\n",
        "# Generate test inputs\n",
        "X_test2 = random.uniform(random.PRNGKey(0), shape=(N_test2,),  minval=-5, maxval=5)\n",
        "\n",
        "# Generate test outputs\n",
        "m_Yt2 = function_true2(X_test2)\n",
        "# or m_Y = function_true2(X\n",
        "y_test2 =  m_Yt2 + sigma*random.normal(random.PRNGKey(1), shape=(N_test2,))\n",
        "\n",
        "\n",
        "# Plot the training and test data together\n",
        "fig, axs = plt.subplots(1,1,figsize=(6, 5), layout=\"constrained\")\n",
        "\n",
        "axs.scatter(x=X_train, y=y_train,  marker = '.', linewidths = 0.1, label=\"train\")\n",
        "axs.scatter(x=X_test2, y=y_test2,  marker = '.', linewidths = 0.1, label=\"test\")\n",
        "axs.legend(loc=\"upper left\")\n",
        "axs.set(xlabel=\"x\", ylabel=\"y\")"
      ],
      "metadata": {
        "id": "XLW9BR9EgNag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "x_test2_scaled = x_scaler.transform(X_test2[:, None])\n",
        "x_test2_scaled = jnp.array(x_test2_scaled)"
      ],
      "metadata": {
        "id": "0YsV-GTyglpd"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_predictive = Predictive(model=model, guide=map_guide, params=map_params, num_samples=num_samples)\n",
        "map_predictions2 = posterior_predictive(rng_key = rng_key_predict, x=x_test2_scaled, y=None, hidden_dim=D_h)"
      ],
      "metadata": {
        "id": "fIpQzJYBhfDd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_pred = jnp.mean(map_predictions2['y_pred'], axis = 0)\n",
        "lower_pred = jnp.quantile(map_predictions2['y'], q = 0.0275, axis = 0)\n",
        "upper_pred = jnp.quantile(map_predictions2['y'], q = 0.975, axis = 0)\n",
        "x_idx = jnp.argsort(x_test2_scaled[:, 0])\n",
        "\n",
        "# make plots\n",
        "fig, ax = plt.subplots(1, figsize=(6, 6), constrained_layout=True)\n",
        "\n",
        "# plot training data\n",
        "ax.plot(x_test2_scaled[:, 0], y_test2, '.', label = 'Samples for testing')\n",
        "# plot mean prediction\n",
        "# ax.plot(x_test_scaled[:, 0], mean_pred[:, 0], '.')\n",
        "ax.plot(x_test2_scaled[:, 0][x_idx], mean_pred[x_idx], color = \"purple\", ls=\"solid\", lw=2.0, label = 'Prediction mean')\n",
        "# plot true mean\n",
        "ax.plot(x_test2_scaled[:, 0][x_idx], m_Yt2[x_idx], color = \"red\", ls=\"solid\", lw=2.0, label = 'True mean')\n",
        "#plot uncertainty\n",
        "ax.fill_between(x_test2_scaled[:, 0][x_idx], lower_pred[x_idx], upper_pred[x_idx], alpha = 0.2, label = 'Uncertainty')\n",
        "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"MAP\")\n",
        "fig.legend()"
      ],
      "metadata": {
        "id": "emfURFcmh0gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises\n",
        "\n",
        "Notice how MAP inference provides poor, overconfident out of sample predictions. How do MCMC and VI compare?"
      ],
      "metadata": {
        "id": "8jF_4q_5jHlh"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}